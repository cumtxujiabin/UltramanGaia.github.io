<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="机器学习," />










<meta name="description" content="1. 绪论人类的强大 在于 经验 + 创新问题:是否能够让程序根据经验来改进自身以满足需求或外界的变化？机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。经验–&amp;gt;来自数据利用经验的方式–&amp;gt; 模型计算的手段–&amp;gt;从数据中产生模型的方法–&amp;gt;学习算法    Item1 Item2     data Different applications have differen">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="http://ultramangaia.github.io/blog/2017/机器学习.html">
<meta property="og:site_name" content="UltramanGaia&#39;s Blog">
<meta property="og:description" content="1. 绪论人类的强大 在于 经验 + 创新问题:是否能够让程序根据经验来改进自身以满足需求或外界的变化？机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。经验–&amp;gt;来自数据利用经验的方式–&amp;gt; 模型计算的手段–&amp;gt;从数据中产生模型的方法–&amp;gt;学习算法    Item1 Item2     data Different applications have differen">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-01-03T04:28:53.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习">
<meta name="twitter:description" content="1. 绪论人类的强大 在于 经验 + 创新问题:是否能够让程序根据经验来改进自身以满足需求或外界的变化？机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。经验–&amp;gt;来自数据利用经验的方式–&amp;gt; 模型计算的手段–&amp;gt;从数据中产生模型的方法–&amp;gt;学习算法    Item1 Item2     data Different applications have differen">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://ultramangaia.github.io/blog/2017/机器学习.html"/>





  <title>机器学习 | UltramanGaia's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">UltramanGaia's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://ultramangaia.github.io/blog/2017/机器学习.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="UltramanGaia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UltramanGaia's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-27T14:38:27+08:00">
                2017-12-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/课程/" itemprop="url" rel="index">
                    <span itemprop="name">课程</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script></p>
<h1 id="1-绪论"><a href="#1-绪论" class="headerlink" title="1. 绪论"></a>1. 绪论</h1><h3 id="人类的强大-在于-经验-创新"><a href="#人类的强大-在于-经验-创新" class="headerlink" title="人类的强大 在于 经验 + 创新"></a>人类的强大 在于 <strong>经验</strong> + <strong>创新</strong></h3><h2 id="问题-是否能够让程序根据经验来改进自身以满足需求或外界的变化？"><a href="#问题-是否能够让程序根据经验来改进自身以满足需求或外界的变化？" class="headerlink" title="问题:是否能够让程序根据经验来改进自身以满足需求或外界的变化？"></a>问题:是否能够让程序根据<em>经验</em>来改进自身以满足需求或外界的变化？</h2><h2 id="机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。"><a href="#机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。" class="headerlink" title="机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。"></a>机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。</h2><p>经验–&gt;来自数据<br>利用经验的方式–&gt; 模型<br>计算的手段–&gt;从数据中产生模型的方法–&gt;学习算法</p>
<table>
<thead>
<tr>
<th style="text-align:center">Item1</th>
<th style="text-align:center">Item2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">data</td>
<td style="text-align:center">Different applications have different data.</td>
</tr>
<tr>
<td style="text-align:center">model(function)</td>
<td style="text-align:center">Define the model according to specific problem.</td>
</tr>
<tr>
<td style="text-align:center">loss(prediction)</td>
<td style="text-align:center">Using loss function to evaluation model</td>
</tr>
</tbody>
</table>
<h3 id="貌似现在接触的机器学习大多数都是针对结构化的数据，我们称一条结构化的数据为一条记录或一个样本，记录的集合为数据集。"><a href="#貌似现在接触的机器学习大多数都是针对结构化的数据，我们称一条结构化的数据为一条记录或一个样本，记录的集合为数据集。" class="headerlink" title="貌似现在接触的机器学习大多数都是针对结构化的数据，我们称一条结构化的数据为一条记录或一个样本，记录的集合为数据集。"></a>貌似现在接触的机器学习大多数都是针对结构化的数据，我们称一条结构化的数据为一条记录或一个样本，记录的集合为<strong>数据集</strong>。</h3><h3 id="监督学习原理"><a href="#监督学习原理" class="headerlink" title="监督学习原理"></a>监督学习原理</h3><h4 id="存在某个不为人知的规律，即y与X有关系，我们猜测是满足下面这种形式"><a href="#存在某个不为人知的规律，即y与X有关系，我们猜测是满足下面这种形式" class="headerlink" title="存在某个不为人知的规律，即y与X有关系，我们猜测是满足下面这种形式"></a>存在某个不为人知的规律，即y与X有关系，我们猜测是满足下面这种形式</h4><p>$$y=f(X)=\omega X+b$$</p>
<h4 id="该公式代表了一系列的具体的公式，我们需要通过一些已知的y0和X0的关系来逐步修改公式来逼近真实情况"><a href="#该公式代表了一系列的具体的公式，我们需要通过一些已知的y0和X0的关系来逐步修改公式来逼近真实情况" class="headerlink" title="该公式代表了一系列的具体的公式，我们需要通过一些已知的y0和X0的关系来逐步修改公式来逼近真实情况"></a>该公式代表了一系列的具体的公式，我们需要通过一些已知的y0和X0的关系来逐步修改公式来逼近真实情况</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. define a set of function</span><br><span class="line">2. goodless of function</span><br><span class="line">3. pick the best function</span><br></pre></td></tr></table></figure>
<h3 id="分类与回归"><a href="#分类与回归" class="headerlink" title="分类与回归"></a>分类与回归</h3><p><strong>分类</strong>：预测的是离散值<br><strong>回归</strong>：预测的是连续值</p>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><h4 id="奥卡姆剃刀-Occam’s-razor"><a href="#奥卡姆剃刀-Occam’s-razor" class="headerlink" title="奥卡姆剃刀(Occam’s razor)"></a>奥卡姆剃刀(Occam’s razor)</h4><p>若有多个假设与观察一致，则选最简单的那一个。</p>
<p>机器学习的目标是使学得的模型能很好地适用于<em>新样本</em>。</p>
<h1 id="2-模型评估与选择"><a href="#2-模型评估与选择" class="headerlink" title="2. 模型评估与选择"></a>2. 模型评估与选择</h1><h3 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h3><p>error rate=错误率：分类错误的样本数占样本总数的比例<br>accuracy=精度：分类正确的样本数占样本总数的比例<br>error=误差：学习器的实际预测输出与样本的真实输出之间的差异<br>training error=训练误差/empirical error=经验误差：在训练集上的误差<br>generalization error=泛化误差：在新样本上的误差</p>
<h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><h4 id="留出法hold-out"><a href="#留出法hold-out" class="headerlink" title="留出法hold-out"></a>留出法hold-out</h4><p>直接将数据集划分成互斥的两个集合<br>一般需要若干次随机划分，取平均值。2/3～4/5样本用于训练</p>
<h4 id="交叉验证法corss-validation"><a href="#交叉验证法corss-validation" class="headerlink" title="交叉验证法corss validation"></a>交叉验证法corss validation</h4><p>将数据集划分成k个大小相似的互斥子集，k-1个作为训练集，另外1个作为测试集<br>k折交叉验证 k-fold cross validation</p>
<h5 id="10次10折交叉验证"><a href="#10次10折交叉验证" class="headerlink" title="10次10折交叉验证"></a>10次10折交叉验证</h5><h5 id="留一法Leave-One-out"><a href="#留一法Leave-One-out" class="headerlink" title="留一法Leave-One-out"></a>留一法Leave-One-out</h5><p>n个样本，分成n份的交叉验证。</p>
<h4 id="自助法-bootstrapping"><a href="#自助法-bootstrapping" class="headerlink" title="自助法 bootstrapping"></a>自助法 bootstrapping</h4><p>自助采样法(bootstrap samping)<br>包外估计(out-of-bag estimate)<br>自助法对数据集较小、难以有效划分训练集和测试集时很有用。但是，改变了初始数据集的分布，引入了估计偏差。</p>
<h3 id="调参与最终模型"><a href="#调参与最终模型" class="headerlink" title="调参与最终模型"></a>调参与最终模型</h3><p>parameter tuning=调参<br>validation set=验证集</p>
<h3 id="性能度量performance-measure"><a href="#性能度量performance-measure" class="headerlink" title="性能度量performance measure"></a>性能度量performance measure</h3><p>回归问题中最常见的性能度量是均方误差mean squared error</p>
<h3 id="查准率-精确率precision、查全率-召回率recall、F1"><a href="#查准率-精确率precision、查全率-召回率recall、F1" class="headerlink" title="查准率=精确率precision、查全率=召回率recall、F1"></a>查准率=精确率precision、查全率=召回率recall、F1</h3><table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">预测正例</td>
<td style="text-align:center">预测反例</td>
</tr>
<tr>
<td style="text-align:center">真实正例</td>
<td style="text-align:center">真正例TP</td>
<td style="text-align:center">假反例FN</td>
</tr>
<tr>
<td style="text-align:center">真实反例</td>
<td style="text-align:center">假正例FP</td>
<td style="text-align:center">真反例TN</td>
</tr>
</tbody>
</table>
<p>$$P = { TP \over TP + FN}.$$<br>$$R= { TP \over TP+FN}.$$</p>
<p>Break-Even Point平衡点：查准率=查全率 的取值。</p>
<h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><p>$$F1 = { {2 \times P \times R} \over {P + R} } = { {2 \times TP} \over { 样例总数 + TP -TN } }$$</p>
<h4 id="当对于-P-和-R-有侧重时"><a href="#当对于-P-和-R-有侧重时" class="headerlink" title="当对于 P 和 R 有侧重时"></a>当对于 P 和 R 有侧重时</h4><p>$$F_\beta=\frac{(1 + \beta^2) \times P \times R}{(\beta^2 \times P) + R}$$</p>
<ul>
<li>当 $\beta = 1$，退化为F1</li>
<li>当 $\beta &gt; 1$，查全率比较重要</li>
<li>当 $\beta &lt; 1$，查准率比较重要</li>
</ul>
<h3 id="ROC-与-AUC"><a href="#ROC-与-AUC" class="headerlink" title="ROC 与 AUC"></a>ROC 与 AUC</h3><h4 id="ROC-Receiver-Operating-Characteristic-受试者工作特征"><a href="#ROC-Receiver-Operating-Characteristic-受试者工作特征" class="headerlink" title="ROC = Receiver Operating Characteristic = 受试者工作特征"></a>ROC = Receiver Operating Characteristic = 受试者工作特征</h4><p>真正例率<br>$$ TPR = {TP \over TP + FN}$$<br>假正例率<br>$$ FPR = {FP \over TN + FP}$$</p>
<h4 id="AUC-Area-Under-ROC-Curve"><a href="#AUC-Area-Under-ROC-Curve" class="headerlink" title="AUC = Area Under ROC Curve"></a>AUC = Area Under ROC Curve</h4><p>ROC底下面积，越大越好</p>
<h3 id="代价矩阵"><a href="#代价矩阵" class="headerlink" title="代价矩阵"></a>代价矩阵</h3><p>由于可能对于分类错误的情况会造成不同的代价，可以引入代价矩阵，将最优化变为希望最小化<em>总体代价</em></p>
<h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h4><p>$$f(x)=\omega_1x_1+\omega_2x_2+…+\omega_dx_d+b$$<br>向量形式<br>$$f(x)=\omega^Tx+b$$</p>
<h1 id="复习提纲"><a href="#复习提纲" class="headerlink" title="复习提纲"></a>复习提纲</h1><h2 id="BasicConcept-都考"><a href="#BasicConcept-都考" class="headerlink" title="BasicConcept 都考"></a>BasicConcept 都考</h2><ol>
<li>Basic Concepts about Machine Learning<br>Q1. What is Machine Learning?<br>A1. Mchine Learning compose of three parts:<ul>
<li>Data</li>
<li>Model(function)</li>
<li>Loss(prediction)<br>|Item1|Item2|<br>|:-:|:-:|<br>|data|Different applications have different data.|<br>|model(function)|Define the model according to specific problem.|<br>|loss(prediction)|Using loss function to evaluation model|</li>
</ul>
</li>
</ol>
<p>Supervised learning is the machine learning task of inferring a function from labeled training data<br>监督学习是从标记训练数据推断函数的机器学习任务。</p>
<ol>
<li><p>Linear Regression<br>Simple linear regression describes the linear relationship between a predictor variable, plotted on the x-axis, and a response variable, plotted on the y-axis<br>简单线性回归描述了预测变量之间的线性关系，绘制在x轴上，并在y轴上绘制一个响应变量。</p>
</li>
<li><p>Closed-form solution 封闭解</p>
</li>
</ol>
<p>许多矩阵是不可逆的。<br>大矩阵的逆需要巨大的内存。<br>逆运算需要O（m 3）来计算。</p>
<p>所以使用 Gradient Descent 梯度下降</p>
<ol>
<li>Gradient Descent</li>
</ol>
<p>Learning rate η has a large impact on convergence<br>Too large η → oscillatory and may even diverge<br>Too small η → too slow to converge<br>学习率η对收敛的影响较大<br>太大的η→振荡，甚至发散<br>太小η→太慢收敛</p>
<p>Adaptive learning rate(For example):<br>Set larger learning rate at the begining<br>Use relatively smaller learning rate in the later epochs<br>Decrease the learning rate: η t+1 = η t / t+1<br>自适应学习率（例如）：<br>一开始就设置较大的学习率<br>在以后的时代使用相对较小的学习率<br>减少学习率：η t+1 =η t / t+1</p>
<h2 id="Linear-Classfication：-Dual-SVM及其推导不考"><a href="#Linear-Classfication：-Dual-SVM及其推导不考" class="headerlink" title="Linear Classfication： Dual SVM及其推导不考"></a>Linear Classfication： Dual SVM及其推导不考</h2><ol>
<li>Linear Classification<br>f(xi)<br>≥ 0 yi = +1<br>&lt; 0 yi = −1</li>
</ol>
<p>i.e. yi f(xi) &gt; 0 for a correct classification</p>
<ol>
<li>Support Vector Machine<br>What’s a Good Decision Boundary<br>什么是好的决策边界？<br>Maximum margin solution: most stable under perturbations of the inputs<br>最大间隔解：在输入扰动下最稳定</li>
</ol>
<p>Select two parallel hyperplanes that separate the two classes of data and let the distance between them as large as possible<br>The region bounded by these two hyperplanes is called the ”margin”<br>选择两个平行的超平面分开两类数据，让他们之间的距离尽可能大<br>该区域范围内的两平面称为“边缘”</p>
<p>In general there is a trade off between the margin and the number of mistakes on the training data</p>
<p>软间隔 soft margin<br>松弛变量 slack variables</p>
<ol>
<li>Stochastic Gradient Descent<br>True gradient descent is a batch algorithm, slow but sure<br>真正的梯度下降是一个批处理算法，缓慢但肯定<br>Information is redundant<br>Sufficient samples means we can afford more frequent, noisy updates<br>Never-ending stream means we should not wait for all data<br>Tracking non-stationary data means that the target is moving<br>信息是多余的<br>充足的样本意味着我们能负担得起更频繁、更嘈杂的更新。<br>永不结束流意味着我们不应该等待所有数据。<br>跟踪非平稳数据意味着目标在移动。</li>
</ol>
<p>Better for large datasets and often faster convergence<br>Hard to reach high accuracy<br>Best classical methods can not handle stochastic approximation<br>Theoretical definitions for convergence not as well defined<br>对于大型数据集，往往更快收敛<br>难以达到高精度<br>最好的经典方法不能处理随机逼近。<br>收敛性定义不明确的理论定义</p>
<h4 id="SGD的好处"><a href="#SGD的好处" class="headerlink" title="SGD的好处"></a>SGD的好处</h4><ol>
<li>Gradient is easy to calculate (instantaneous)</li>
<li>Less prone to local minima</li>
<li>Small memory footprint</li>
<li>Get to a reasonable solution quickly</li>
<li>Works for non-stationary environments as well as online settings</li>
<li>Can be used for more complex models and error surfaces<br>梯度很容易计算（瞬时）<br>不易发生局部极小<br>小内存占用<br>迅速找到合理的解决办法<br>适用于非平稳环境以及在线设置。<br>可用于更复杂的模型和错误表面。<h4 id="Is-convergence-necessary"><a href="#Is-convergence-necessary" class="headerlink" title="Is convergence necessary?"></a>Is convergence necessary?</h4>Non-stationary: convergence may not be required<br>Stationary: learning rate should decrease with time<br>Robbins-Monroe sequence is adequate ηt = 1/t<h4 id="Mini-batch-Stochastic-Gradient-Descent"><a href="#Mini-batch-Stochastic-Gradient-Descent" class="headerlink" title="Mini-batch Stochastic Gradient Descent"></a>Mini-batch Stochastic Gradient Descent</h4>小批量梯度下降<h2 id="SGD-Recommenddations"><a href="#SGD-Recommenddations" class="headerlink" title="SGD Recommenddations"></a>SGD Recommenddations</h2></li>
<li>Randomly shuffle training examples<br>Although theory says you should randomly pick examples, it is easier to make a pass through your training set sequentially<br>Shuffling before each iteration eliminates the effect of order<br>随机洗牌训练示例<br>虽然理论说你应该随机挑选例子，但是更容易通过你的训练集顺序。<br>每次迭代之前进行洗牌消除了订单的影响。</li>
<li>Monitor both training cost and validation error<br>Set aside samples for a decent validation set<br>Compute the objective on the training set and validation set (expensive but better than overfitting or wasting computation)<br>监控培训成本和验证错误<br>为正确的验证集预留样本<br>在训练集和验证集的目的计算（贵，但比过拟合或浪费计算）</li>
<li>Check gradient using finite differences<br>If computation is slightly incorrect can yield erratic and slow algorithm<br>Verify your code by slightly perturbing the parameter and inspecting differences between the two gradients<br>利用有限差分检验梯度<br>如果计算稍不正确，则会产生不稳定和缓慢的算法。<br>通过轻微的扰动参数和检验两者之间的梯度差异，验证你的代码</li>
<li>Experiment with the learning rates using small sample of training set<br>SGD convergence rates are independent from sample size<br>Use traditional optimization algorithms as a reference point<br>用训练样本的小样本进行学习率的实验<br>SGD的收敛速度是独立样本<br>使用传统的优化算法作为参考点</li>
<li>Leverage sparsity of the training examples<br>For very high-dimensional vectors with few non zero coefficients, you only need to update the weight coefficients corresponding to nonzero pattern in x<br>利用训练样本的稀疏性<br>对于很少的非零系数的高维向量，只需更新x中对应于非零模式的权系数。</li>
<li>Use learning rates of the form ηt = η0/(1 + η0λt)<br>Allows you to start from reasonable learning rates determined by testing on a small sample<br>Works well in most situations if the initial point is slightly smaller than best value observed in training sample<br>使用表格ηT = 0 /η学习率（1 + 0ηλT）<br>允许你从一个小样本测试中确定的合理学习率开始。<br>如果初始点略小于训练样本中观察到的最佳值，那么在大多数情况下都能很好地工作。</li>
</ol>
<h2 id="Logistic-Regression-and-Softmax-Regression：-都考"><a href="#Logistic-Regression-and-Softmax-Regression：-都考" class="headerlink" title="Logistic Regression and Softmax Regression： 都考"></a>Logistic Regression and Softmax Regression： 都考</h2><h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><p>还没懂</p>
<h4 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h4><p>还没懂</p>
<h2 id="Multiclass-classification：-Hierarchical-classification-不考"><a href="#Multiclass-classification：-Hierarchical-classification-不考" class="headerlink" title="Multiclass classification： Hierarchical classification 不考"></a>Multiclass classification： Hierarchical classification 不考</h2><p>那不看先</p>
<h2 id="Cross-Validation都考"><a href="#Cross-Validation都考" class="headerlink" title="Cross-Validation都考"></a>Cross-Validation都考</h2><h3 id="Overﬁtting-Underﬁtting-and-Cross-Validation"><a href="#Overﬁtting-Underﬁtting-and-Cross-Validation" class="headerlink" title="Overﬁtting, Underﬁtting and Cross-Validation"></a>Overﬁtting, Underﬁtting and Cross-Validation</h3><h4 id="Problem-of-Model-Validation"><a href="#Problem-of-Model-Validation" class="headerlink" title="Problem of Model Validation"></a>Problem of Model Validation</h4><p>Q1. Why We All Need Validation<br>Business Reasons<br>Need to choose the best model.<br>Measure accuracy/power of selected model.<br>Good to measure ROI of the modeling project.<br>Statistical Reasons<br>Model building techniques are inherently designed to minimize “loss” or “bias”.<br>To an extent, a model will always fit “noise” as well as “signal”.<br>If you just fit a bunch of models on a given dataset and choose the “best” one, it will likely be overly “optimistic”.<br>企业的原因<br>需要选择最好的模型。<br>测量精度/模型功率。<br>很好地度量建模项目的ROI。<br>统计方面的原因<br>模型构建技术本质上是为了最小化“损失”或“偏见”而设计的。<br>在某种程度上，一个模型总是符合“噪声”和“信号”。<br>如果你只是在给定的数据集上选择一堆模型，选择“最佳”，那么它可能过于“乐观”。</p>
<h4 id="Underfitting-and-Overfitting"><a href="#Underfitting-and-Overfitting" class="headerlink" title="Underfitting and Overfitting"></a>Underfitting and Overfitting</h4><p>Underfitting: high bias 偏差大<br>Model cannot capture the underlying trend of the data<br>Overfitting: high variance 方差大<br>Model is too complex to capture the true pattern<br>Model seeks to fit the noise or outlier of the data </p>
<h4 id="Bias-Variance-Tradeoff"><a href="#Bias-Variance-Tradeoff" class="headerlink" title="Bias-Variance Tradeoff"></a>Bias-Variance Tradeoff</h4><p>Error on the dataset used to fit the model can be misleading<br>Doesn’t predict future performance.<br>Too much complexity can diminish model’s accuracy on future data.<br>Sometimes called the Bias-Variance Tradeoff.<br>用于拟合模型的数据集上的错误可能会引起误解。<br>不能预测未来的表现。<br>太多的复杂性会降低模型对未来数据的准确性。<br>有时称为偏差方差权衡。<br>Complex model:<br>Low “bias”:<br>The model fit is good on the training data.<br>The model value is close to the data’s expected value.<br>High “Variance”:<br>Model more likely to make a wrong prediction.<br>复杂的模型：<br>低偏压：<br>模型拟合对训练数据有较好的拟合效果。<br>模型值接近数据的期望值。<br>高“方差”：<br>更有可能做出错误预测的模型。</p>
<h5 id="How-do-we-know-if-we-are-underfitting-or-overfitting"><a href="#How-do-we-know-if-we-are-underfitting-or-overfitting" class="headerlink" title="How do we know if we are underfitting or overfitting?"></a>How do we know if we are underfitting or overfitting?</h5><p>If by increasing capacity we decrease generalization error, then we are underfitting, otherwise we are overfitting.<br>If the error in representing the training set is relatively large and the generalization error is large, then underfitting;<br>如果我们增加容量，降低泛化误差，那么我们欠学习，否则我们过。<br>如果错误代表训练集比较大，产生的误差较大，那么欠；<br>Need to increase capacity (complexity of models).<br>   – If the error in representing the training set is relatively small and the generalization error is large, then overfitting;<br>Need to decrease capacity or increase training set.<br>   – Many features and relatively small training set.<br>   – if you have chosen a large capacity to complement the many features, then you might overfit the data:<br>need to decrease the capacity.<br>需要增加容量（模型的复杂性）。<br>–如果错误代表训练集比较小的泛化误差大，然后过；<br>需要减少能力或增加训练集。<br>-许多功能和相对较小的训练集。<br>–如果你选择了一个大容量为补充的多种特征，那么你可能过度拟合数据：<br>需要降低容量。</p>
<h3 id="Cross-Validation-重要"><a href="#Cross-Validation-重要" class="headerlink" title="Cross-Validation 重要"></a>Cross-Validation 重要</h3><h4 id="Training-Data-Validation-Data-Testing-Data"><a href="#Training-Data-Validation-Data-Testing-Data" class="headerlink" title="Training Data, Validation Data, Testing Data"></a>Training Data, Validation Data, Testing Data</h4><h4 id="Performance-Report"><a href="#Performance-Report" class="headerlink" title="Performance Report"></a>Performance Report</h4><h4 id="Parameter-Tuning"><a href="#Parameter-Tuning" class="headerlink" title="Parameter Tuning"></a>Parameter Tuning</h4><h4 id="K-Fold-Cross-Validation"><a href="#K-Fold-Cross-Validation" class="headerlink" title="K-Fold Cross-Validation"></a>K-Fold Cross-Validation</h4><h2 id="PCA：只考1-16页，即Another-Point-of-View-之后都不考"><a href="#PCA：只考1-16页，即Another-Point-of-View-之后都不考" class="headerlink" title="PCA：只考1-16页，即Another Point of View 之后都不考"></a>PCA：只考1-16页，即Another Point of View 之后都不考</h2><h2 id="Ensemble-Methods：Adaboost-GBDT不考"><a href="#Ensemble-Methods：Adaboost-GBDT不考" class="headerlink" title="Ensemble Methods：Adaboost: GBDT不考"></a>Ensemble Methods：Adaboost: GBDT不考</h2><h2 id="Ensemble-Methods-Decision-Tree-and-Random-Forest-：-Random-forest不考"><a href="#Ensemble-Methods-Decision-Tree-and-Random-Forest-：-Random-forest不考" class="headerlink" title="Ensemble Methods(Decision Tree and Random Forest)： Random forest不考"></a>Ensemble Methods(Decision Tree and Random Forest)： Random forest不考</h2><h2 id="Clustering：-Hierarchical-Agglomerative-Clustering不考"><a href="#Clustering：-Hierarchical-Agglomerative-Clustering不考" class="headerlink" title="Clustering： Hierarchical Agglomerative Clustering不考"></a>Clustering： Hierarchical Agglomerative Clustering不考</h2><ol>
<li>Clustering</li>
<li>K-Means Clustering</li>
<li>Hierarchical Agglomerative Clustering 不考</li>
<li>Conclusion</li>
</ol>
<h2 id="Recommendation-system：只考问题定义：什么叫Collaborative-Filtering，Recommendation有哪几种典型方法及各自优缺点，Cold-start-Recommendation概念；不考方法，方法在课程项目中体现。"><a href="#Recommendation-system：只考问题定义：什么叫Collaborative-Filtering，Recommendation有哪几种典型方法及各自优缺点，Cold-start-Recommendation概念；不考方法，方法在课程项目中体现。" class="headerlink" title="Recommendation system：只考问题定义：什么叫Collaborative Filtering，Recommendation有哪几种典型方法及各自优缺点，Cold-start Recommendation概念；不考方法，方法在课程项目中体现。"></a>Recommendation system：只考问题定义：什么叫Collaborative Filtering，Recommendation有哪几种典型方法及各自优缺点，Cold-start Recommendation概念；不考方法，方法在课程项目中体现。</h2><p>Recommender System applies statistical and knowledge discovery techniques to the problem of making product recommendations.<br>推荐系统将统计和知识发现技术应用于产品推荐问题。<br>Advantages of recommender systems:<br>Improve conversion rate: Help customers find a product she/he wants to buy.<br>增加营业额<br>Cross-selling: Suggest additional products.<br>促销，附加商品<br>Improve customer loyalty: Create a value-added relationship. 提高用户忠诚度</p>
<p>Q1. 什么叫Collaborative Filtering<br>Make automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many other users (collaboration).<br>通过收集偏好或从许多其他用户那里获取信息来进行用户兴趣的自动预测（过滤）（协作）。<br>A1. Collaborative filtering， 即协同过滤。 使用某人的行为behavior来预测其它人会做什么。协同过滤就是基于邻域的算法，分为基于用户的协同过滤算法UserCF和基于物品的协同过滤算法ItemCF。<br>一种是User-Based，即是利用用户与用户之间的相似性，生成最近的邻居，当需要推荐的时候，从最近的邻居中得到推荐得分最高的几篇文章，用作推荐；<br>另外一种是Item-Based，即是基于item之间的关系，针对item来作推荐</p>
<p>Q2. Recommendation有哪几种典型方法及各自优缺点<br>Memory-based CF :utilize the entire user-item database to generate a prediction.</p>
<ul>
<li>User-based CF : find similar users to predict ratings.</li>
<li>Item-based CF : use similar items to predict ratings<br>Model-based CF : build a model from the rating data<br>( Matrix factorization, etc.) and use this model to predict missing ratings.<br>矩阵分解</li>
</ul>
<p>User-based similarity is more dynamic.<br>Precompute user neighbourhood may lead to poor predictions.<br>Item-based similarity is static.<br>Precompute item neighbourhood may provide accurate results.<br>基于用户的相似性更具动态性。<br>预计算用户附近可能导致不良的预测。<br>基于项的相似性是静态的。<br>预计算项目附近可以提供准确的结果。</p>
<p>Memory-Based CF<br>优点：<br>Require minimal knowledge engineering efforts.<br>Users and products are symbols without any internal structure or characteristics.<br>Produce good-enough results in most cases.<br>需要最少的知识工程努力。<br>用户和产品是没有任何内部结构或特性的符号。<br>在大多数情况下产生足够好的结果。<br>缺点：<br>Require a large number of explicit and reliable ratings.<br>Highly time consuming to compute similarity with millions of users &amp; items.<br>需要大量明确可靠的评级。<br>非常耗费时间来计算与数以百万计的用户和项目的相似性。</p>
<p>Model-based CF<br>Models are learned from the underlying data rather than heuristics<br>模型是从底层数据中学习的，而不是启发式的。<br>Models of user ratings (or purchases):<br>用户评级模型（或购买）：<br>Matrix Factorization is the most widely used algorithm.</p>
<p>Comparison between SGD and ALS<br>ALS is easier to parallelize than SGD.<br>ALS converges faster than the SGD.<br>SGD has less storage complexity than ALS.<br>(ALS needs to store the rating matrix R)<br>SGD has less computational complexity than ALS.<br>(ALS needs to compute the matrix-vector multiplication)<br>SGD和ALS的比较<br>ALS是比SGD容易并行化。<br>ALS的SGD更快的收敛速度比。<br>SGD比ALS较少的存储复杂度。<br>（ALS需要存储评级矩阵R）<br>SGD具有较小的计算复杂度比ALS。<br>（ALS需要计算矩阵向量乘法）</p>
<p>推荐系统构建三大方法：基于内容的推荐content-based，协同过滤collaborative filtering，隐语义模型(LFM, latent factor model)推荐</p>
<ul>
<li>基于内容的推荐content-based</li>
<li>协同过滤collaborative filtering</li>
<li>隐语义模型(LFM, latent factor model)推荐</li>
</ul>
<p>Q3. Cold-start Recommendation概念<br>如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意从而愿意使用推荐系统，就是冷启动问题。<br>1）用户冷启动：如何给新用户做个性化推荐<br>2）物品冷启动：如何将新物品推荐给可能对其感兴趣的用户。在新闻网站等时效性很强的网站中非常重要。<br>3）系统冷启动：如何在一个新开发的网站上设计个性化推荐，从而在网站刚发布时就让用户体验到个性化推荐服务。没有用户，只有一些物品信息。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2017/Kali-Linux-工具目录.html" rel="next" title="Kali Linux 工具目录">
                <i class="fa fa-chevron-left"></i> Kali Linux 工具目录
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2017/软件体系架构.html" rel="prev" title="软件体系架构">
                软件体系架构 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="UltramanGaia" />
            
              <p class="site-author-name" itemprop="name">UltramanGaia</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-绪论"><span class="nav-number">1.</span> <span class="nav-text">1. 绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#人类的强大-在于-经验-创新"><span class="nav-number">1.0.1.</span> <span class="nav-text">人类的强大 在于 经验 + 创新</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#问题-是否能够让程序根据经验来改进自身以满足需求或外界的变化？"><span class="nav-number">1.1.</span> <span class="nav-text">问题:是否能够让程序根据经验来改进自身以满足需求或外界的变化？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。"><span class="nav-number">1.2.</span> <span class="nav-text">机器学习致力于研究通过计算的手段，利用经验来改进自己的性能。</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#貌似现在接触的机器学习大多数都是针对结构化的数据，我们称一条结构化的数据为一条记录或一个样本，记录的集合为数据集。"><span class="nav-number">1.2.1.</span> <span class="nav-text">貌似现在接触的机器学习大多数都是针对结构化的数据，我们称一条结构化的数据为一条记录或一个样本，记录的集合为数据集。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习原理"><span class="nav-number">1.2.2.</span> <span class="nav-text">监督学习原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#存在某个不为人知的规律，即y与X有关系，我们猜测是满足下面这种形式"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">存在某个不为人知的规律，即y与X有关系，我们猜测是满足下面这种形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#该公式代表了一系列的具体的公式，我们需要通过一些已知的y0和X0的关系来逐步修改公式来逼近真实情况"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">该公式代表了一系列的具体的公式，我们需要通过一些已知的y0和X0的关系来逐步修改公式来逼近真实情况</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类与回归"><span class="nav-number">1.2.3.</span> <span class="nav-text">分类与回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无监督学习"><span class="nav-number">1.2.4.</span> <span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#聚类"><span class="nav-number">1.2.5.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#奥卡姆剃刀-Occam’s-razor"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">奥卡姆剃刀(Occam’s razor)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-模型评估与选择"><span class="nav-number">2.</span> <span class="nav-text">2. 模型评估与选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#经验误差与过拟合"><span class="nav-number">2.0.1.</span> <span class="nav-text">经验误差与过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评估方法"><span class="nav-number">2.0.2.</span> <span class="nav-text">评估方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#留出法hold-out"><span class="nav-number">2.0.2.1.</span> <span class="nav-text">留出法hold-out</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉验证法corss-validation"><span class="nav-number">2.0.2.2.</span> <span class="nav-text">交叉验证法corss validation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#10次10折交叉验证"><span class="nav-number">2.0.2.2.1.</span> <span class="nav-text">10次10折交叉验证</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#留一法Leave-One-out"><span class="nav-number">2.0.2.2.2.</span> <span class="nav-text">留一法Leave-One-out</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自助法-bootstrapping"><span class="nav-number">2.0.2.3.</span> <span class="nav-text">自助法 bootstrapping</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调参与最终模型"><span class="nav-number">2.0.3.</span> <span class="nav-text">调参与最终模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#性能度量performance-measure"><span class="nav-number">2.0.4.</span> <span class="nav-text">性能度量performance measure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查准率-精确率precision、查全率-召回率recall、F1"><span class="nav-number">2.0.5.</span> <span class="nav-text">查准率=精确率precision、查全率=召回率recall、F1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F1"><span class="nav-number">2.0.6.</span> <span class="nav-text">F1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#当对于-P-和-R-有侧重时"><span class="nav-number">2.0.6.1.</span> <span class="nav-text">当对于 P 和 R 有侧重时</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ROC-与-AUC"><span class="nav-number">2.0.7.</span> <span class="nav-text">ROC 与 AUC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ROC-Receiver-Operating-Characteristic-受试者工作特征"><span class="nav-number">2.0.7.1.</span> <span class="nav-text">ROC = Receiver Operating Characteristic = 受试者工作特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AUC-Area-Under-ROC-Curve"><span class="nav-number">2.0.7.2.</span> <span class="nav-text">AUC = Area Under ROC Curve</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价矩阵"><span class="nav-number">2.0.8.</span> <span class="nav-text">代价矩阵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性模型"><span class="nav-number">2.1.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本形式"><span class="nav-number">2.1.0.1.</span> <span class="nav-text">基本形式</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#复习提纲"><span class="nav-number">3.</span> <span class="nav-text">复习提纲</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#BasicConcept-都考"><span class="nav-number">3.1.</span> <span class="nav-text">BasicConcept 都考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Classfication：-Dual-SVM及其推导不考"><span class="nav-number">3.2.</span> <span class="nav-text">Linear Classfication： Dual SVM及其推导不考</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD的好处"><span class="nav-number">3.2.0.1.</span> <span class="nav-text">SGD的好处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Is-convergence-necessary"><span class="nav-number">3.2.0.2.</span> <span class="nav-text">Is convergence necessary?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mini-batch-Stochastic-Gradient-Descent"><span class="nav-number">3.2.0.3.</span> <span class="nav-text">Mini-batch Stochastic Gradient Descent</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD-Recommenddations"><span class="nav-number">3.3.</span> <span class="nav-text">SGD Recommenddations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression-and-Softmax-Regression：-都考"><span class="nav-number">3.4.</span> <span class="nav-text">Logistic Regression and Softmax Regression： 都考</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">3.4.0.1.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax-Regression"><span class="nav-number">3.4.0.2.</span> <span class="nav-text">Softmax Regression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiclass-classification：-Hierarchical-classification-不考"><span class="nav-number">3.5.</span> <span class="nav-text">Multiclass classification： Hierarchical classification 不考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-Validation都考"><span class="nav-number">3.6.</span> <span class="nav-text">Cross-Validation都考</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Overﬁtting-Underﬁtting-and-Cross-Validation"><span class="nav-number">3.6.1.</span> <span class="nav-text">Overﬁtting, Underﬁtting and Cross-Validation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem-of-Model-Validation"><span class="nav-number">3.6.1.1.</span> <span class="nav-text">Problem of Model Validation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Underfitting-and-Overfitting"><span class="nav-number">3.6.1.2.</span> <span class="nav-text">Underfitting and Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias-Variance-Tradeoff"><span class="nav-number">3.6.1.3.</span> <span class="nav-text">Bias-Variance Tradeoff</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#How-do-we-know-if-we-are-underfitting-or-overfitting"><span class="nav-number">3.6.1.3.1.</span> <span class="nav-text">How do we know if we are underfitting or overfitting?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Validation-重要"><span class="nav-number">3.6.2.</span> <span class="nav-text">Cross-Validation 重要</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-Data-Validation-Data-Testing-Data"><span class="nav-number">3.6.2.1.</span> <span class="nav-text">Training Data, Validation Data, Testing Data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Performance-Report"><span class="nav-number">3.6.2.2.</span> <span class="nav-text">Performance Report</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parameter-Tuning"><span class="nav-number">3.6.2.3.</span> <span class="nav-text">Parameter Tuning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-Fold-Cross-Validation"><span class="nav-number">3.6.2.4.</span> <span class="nav-text">K-Fold Cross-Validation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PCA：只考1-16页，即Another-Point-of-View-之后都不考"><span class="nav-number">3.7.</span> <span class="nav-text">PCA：只考1-16页，即Another Point of View 之后都不考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble-Methods：Adaboost-GBDT不考"><span class="nav-number">3.8.</span> <span class="nav-text">Ensemble Methods：Adaboost: GBDT不考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble-Methods-Decision-Tree-and-Random-Forest-：-Random-forest不考"><span class="nav-number">3.9.</span> <span class="nav-text">Ensemble Methods(Decision Tree and Random Forest)： Random forest不考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering：-Hierarchical-Agglomerative-Clustering不考"><span class="nav-number">3.10.</span> <span class="nav-text">Clustering： Hierarchical Agglomerative Clustering不考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recommendation-system：只考问题定义：什么叫Collaborative-Filtering，Recommendation有哪几种典型方法及各自优缺点，Cold-start-Recommendation概念；不考方法，方法在课程项目中体现。"><span class="nav-number">3.11.</span> <span class="nav-text">Recommendation system：只考问题定义：什么叫Collaborative Filtering，Recommendation有哪几种典型方法及各自优缺点，Cold-start Recommendation概念；不考方法，方法在课程项目中体现。</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">UltramanGaia</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
